{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "V5_BN_DL_A1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaJHe-E_AyjG",
        "colab_type": "text"
      },
      "source": [
        "Import modules\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vukam595UdLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py \n",
        "import random\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jjgQpwTA9vL",
        "colab_type": "text"
      },
      "source": [
        "Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzUsU1_MU4kD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with h5py.File('/content/drive/My Drive/DL/Assignment-1-Dataset/train_128.h5','r') as H: \n",
        "    data = np.copy(H['data'])\n",
        "with h5py.File('/content/drive/My Drive/DL/Assignment-1-Dataset/train_label.h5','r') as H: \n",
        "    label = np.copy(H['label']) \n",
        "with h5py.File('/content/drive/My Drive/DL/Assignment-1-Dataset/test_128.h5','r') as H: \n",
        "    test = np.copy(H['data']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ4lL6b_ftdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks5Ab2UwFBNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# no need to transform as it was transform in crossEntropyLossFunc\n",
        "def labelToneHot(label):\n",
        "    temp=np.zeros(10)\n",
        "    temp[label]=1\n",
        "    return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Orn-akWlVTGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plt.imshow(data.reshape(data.shape[0],8,-1)[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TG0jmXHBYw9",
        "colab_type": "text"
      },
      "source": [
        "Define the Activation class tool"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-DG4XkXanDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Activation(object):\n",
        "    def __tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def __tanh_deriv(self, a):\n",
        "        return 1.0 - a**2\n",
        "\n",
        "    def __logistic(self, x):\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "    def __logistic_deriv(self, a):\n",
        "        return  a * (1 - a )\n",
        "    #relu \n",
        "    def __relu(self,x):\n",
        "        return np.maximum(0,x)\n",
        "    #relu derivation\n",
        "    def __relu_deriv(self,a):\n",
        "        y = (a > 0) * 1\n",
        "        return y\n",
        "    #leakyrelu\n",
        "    def __leakyrelu(self,x):\n",
        "        return np.where(x > 0, x, x * 0.01)   \n",
        "    #leakyrelu derivation\n",
        "    def __leakyrelu_deriv(self,x):\n",
        "        return np.where(x > 0, 1, 0.01)        \n",
        "\n",
        "    # no need to use it was contained in loss function\n",
        "    def __softmax(self,X):\n",
        "        expZ = np.exp(X - np.max(X))\n",
        "        return expZ/sum(expZ)\n",
        "    # softmax derivation (abandon)\n",
        "    def __softmax_deriv(self,a):\n",
        "        x=softmax(a)\n",
        "        s=x.reshape(-1,1)\n",
        "        return (np.diagflat(s) - np.dot(s, s.T))\n",
        "    \n",
        "    def __init__(self,activation='tanh'):\n",
        "        if activation == 'logistic':\n",
        "            self.f = self.__logistic\n",
        "            self.f_deriv = self.__logistic_deriv\n",
        "        elif activation == 'tanh':\n",
        "            self.f = self.__tanh\n",
        "            self.f_deriv = self.__tanh_deriv\n",
        "        elif activation == 'relu':\n",
        "            self.f = self.__relu\n",
        "            self.f_deriv = self.__relu_deriv\n",
        "        elif activation == 'softmax':\n",
        "            self.f = self.__softmax\n",
        "            self.f_deriv = self.__softmax_deriv\n",
        "        elif activation == 'leakyrelu':\n",
        "            self.f = self.__leakyrelu\n",
        "            self.f_deriv = self.__leakyrelu_deriv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1udYHj_dCBb2",
        "colab_type": "text"
      },
      "source": [
        "defien the optimator class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYLU2ynNotF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OPT:\n",
        "   def __init__(self, name, momentum=0.9,rho2=0.999,rho1=0.9):\n",
        "       self.name=name\n",
        "       self.momentum=momentum\n",
        "       self.rho2=rho2\n",
        "       self.rho1=rho1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_x8j8-ia3hO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HiddenLayer(object):    \n",
        "    def __init__(self,n_in, n_out,\n",
        "                 activation_last_layer='tanh',activation='tanh', W=None, b=None): #activation_last_layer current active  activation: current+1 layer active\n",
        "        \"\"\"\n",
        "        Typical hidden layer of a MLP: units are fully-connected and have\n",
        "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
        "        and the bias vector b is of shape (n_out,).\n",
        "\n",
        "        NOTE : The nonlinearity used here is tanh\n",
        "\n",
        "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
        "\n",
        "        :type n_in: int\n",
        "        :param n_in: dimensionality of input\n",
        "\n",
        "        :type n_out: int\n",
        "        :param n_out: number of hidden units\n",
        "\n",
        "        :type activation: string\n",
        "        :param activation: Non linearity to be applied in the hidden\n",
        "                           layer\n",
        "        \"\"\"\n",
        "        self.input=None\n",
        "        self.activation=None\n",
        "        if activation:\n",
        "            self.activation=Activation(activation).f\n",
        "        \n",
        "        # activation deriv of last layer\n",
        "        self.activation_deriv=None\n",
        "        if activation_last_layer:\n",
        "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
        "        # initialize the weights and bias\n",
        "        self.W = np.random.uniform(\n",
        "                low=-np.sqrt(6. / (n_in + n_out)),\n",
        "                high=np.sqrt(6. / (n_in + n_out)),\n",
        "                size=(n_in, n_out)\n",
        "            )\n",
        "        if activation == 'logistic':\n",
        "            self.W *= 4\n",
        "        if activation == 'relu':\n",
        "            self.W = np.random.uniform(\n",
        "                low=-np.sqrt(6. / ( n_out)),\n",
        "                high=np.sqrt(6. / ( n_out)),\n",
        "                size=(n_in, n_out)\n",
        "                )\n",
        "\n",
        "        self.b = np.zeros(n_out,)\n",
        "        \n",
        "        #for normal mode\n",
        "        self.grad_W = np.zeros(self.W.shape)\n",
        "        self.grad_b = np.zeros(self.b.shape)\n",
        "\n",
        "        # for momentum\n",
        "        self.grad_W_past = np.zeros(self.W.shape)\n",
        "        self.grad_b_past = np.zeros(self.b.shape)\n",
        "        # for adam\n",
        "        self.W_v= np.zeros(self.W.shape)\n",
        "        self.b_v= np.zeros(self.b.shape)\n",
        "        self.W_m= np.zeros(self.W.shape)\n",
        "        self.b_m= np.zeros(self.b.shape)\n",
        "\n",
        "        #for dropout\n",
        "        self.dropW=None\n",
        "        self.M=None\n",
        "        self.output_layer=False\n",
        "        self.dropOut=True\n",
        "        self.delta=None\n",
        "\n",
        "        #FOR BN\n",
        "        self.cache=None\n",
        "        self.gamma=np.random.uniform(\n",
        "                low=-np.sqrt(6. / (n_in + n_out)),\n",
        "                high=np.sqrt(6. / (n_in + n_out)),\n",
        "                size=(n_out,)\n",
        "            )\n",
        "        self.beta=np.zeros(n_out,)\n",
        "        self.grad_gamma=np.zeros(self.gamma.shape)\n",
        "        self.grad_beta=np.zeros(self.beta.shape)\n",
        "        self.mean=None\n",
        "        self.variance=None\n",
        "        #for BN momentum\n",
        "        self.grad_gamma_past=np.zeros(self.gamma.shape)\n",
        "        self.grad_beta_past=np.zeros(self.beta.shape)\n",
        "        #for BN adam\n",
        "        self.gamma_v= np.zeros(self.gamma.shape)\n",
        "        self.beta_v= np.zeros(self.beta.shape)\n",
        "        self.gamma_m= np.zeros(self.gamma.shape)\n",
        "        self.beta_m= np.zeros(self.beta.shape)\n",
        "\n",
        "    def forward(self,input,isTest=False,p=0.5):\n",
        "        '''\n",
        "        :type input: numpy.array\n",
        "        :param input: a symbolic tensor of shape (n_in,)\n",
        "        '''\n",
        "        lin_output=None\n",
        "        if self.dropOut and self.output_layer == False and isTest==False:\n",
        "            # print('FP dropout layer',self.W.shape,self.b.shape)\n",
        "            self.dropW,self.dropB=self.Dropout(self.W,self.b,p)\n",
        "            lin_output =np.dot(input,self.dropW)+self.dropB\n",
        "        else:\n",
        "            # print('FP output layer',self.W.shape,self.b.shape)\n",
        "            lin_output = np.dot(input, self.W) + self.b\n",
        "\n",
        "\n",
        "        ## we put BN func here before the activation\n",
        "        if isTest ==False:\n",
        "            bn_output,self.cache,mean,variance=self.batchnorm_forward(lin_output,self.gamma,self.beta)\n",
        "            if self.mean is None:\n",
        "                self.mean=mean\n",
        "            else:\n",
        "                self.mean=self.mean*0.9 + 0.1 * mean\n",
        "            if self.variance is None:\n",
        "                self.variance=variance\n",
        "            else:\n",
        "                self.variance=self.variance*0.9 + 0.1 * variance\n",
        "        else:\n",
        "            bn_output=self.batchnorm_forward_test(lin_output,self.gamma,self.beta,self.mean,self.variance)\n",
        "\n",
        "        self.output = (\n",
        "            bn_output if self.activation is None\n",
        "            else self.activation(bn_output)\n",
        "        )\n",
        "        self.input=input\n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, delta,p=0.5, output_layer=False):\n",
        "        # print('a',np.atleast_2d(self.input).T.dot(np.atleast_2d(delta)))\n",
        "\n",
        "        self.output_layer=output_layer\n",
        "        delta_return=None\n",
        "\n",
        "        #we put BN BP here \n",
        "        delta,self.grad_gamma,self.grad_beta=self.batchnorm_backward(delta,self.cache)\n",
        "\n",
        "\n",
        "        #dropout BP here\n",
        "        if self.dropOut and self.output_layer ==False:\n",
        "            # print('BP dropout layer',self.W.shape,self.b.shape)\n",
        "            self.delta=delta * self.M/p\n",
        "        else:\n",
        "            # print('BP output layer',self.W.shape,self.b.shape)\n",
        "            self.delta=delta\n",
        "\n",
        "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(self.delta))   # input(64*32)   delta(64*10) \n",
        "        self.grad_b = np.sum(self.delta,axis=0)  #(10,)\n",
        "\n",
        "        if self.activation_deriv:\n",
        "            # W(32*10)  \n",
        "            delta_return = self.delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
        "\n",
        "        return delta_return\n",
        "\n",
        "    def batchnorm_forward_test(self,x,gamma,beta,mean,variance,eps=1e-5):\n",
        "        N, D = x.shape\n",
        "        # #step1: calculate mean\n",
        "        # mu = 1./N * np.sum(x, axis = 0)\n",
        "        #step2: subtract mean vector of every trainings example\n",
        "        xmu = x - mean\n",
        "        #step3: following the lower branch - calculation denominator\n",
        "        sq = xmu ** 2\n",
        "        # #step4: calculate variance\n",
        "        # var = 1./N * np.sum(sq, axis = 0)\n",
        "        #step5: add eps for numerical stability, then sqrt\n",
        "        sqrtvar = np.sqrt(variance + eps)\n",
        "        #step6: invert sqrtwar\n",
        "        ivar = 1./sqrtvar\n",
        "        #step7: execute normalization\n",
        "        xhat = xmu * ivar\n",
        "        #step8: Nor the two transformation steps\n",
        "        gammax = gamma * xhat\n",
        "        #step9\n",
        "        out = gammax + beta\n",
        "        # #store intermediate\n",
        "        # cache = (xhat,gamma,xmu,ivar,sqrtvar,var,eps)\n",
        "        return out\n",
        "\n",
        "\n",
        "    def batchnorm_forward(self,x, gamma, beta, eps=1e-5):\n",
        "        N, D = x.shape\n",
        "        #step1: calculate mean\n",
        "        mu = 1./N * np.sum(x, axis = 0)\n",
        "        #step2: subtract mean vector of every trainings example\n",
        "        xmu = x - mu\n",
        "        #step3: following the lower branch - calculation denominator\n",
        "        sq = xmu ** 2\n",
        "        #step4: calculate variance\n",
        "        var = 1./N * np.sum(sq, axis = 0)\n",
        "        #step5: add eps for numerical stability, then sqrt\n",
        "        sqrtvar = np.sqrt(var + eps)\n",
        "        #step6: invert sqrtwar\n",
        "        ivar = 1./sqrtvar\n",
        "        #step7: execute normalization\n",
        "        xhat = xmu * ivar\n",
        "        #step8: Nor the two transformation steps\n",
        "        gammax = gamma * xhat\n",
        "        #step9\n",
        "        out = gammax + beta\n",
        "        #store intermediate\n",
        "        cache = (xhat,gamma,xmu,ivar,sqrtvar,var,eps)\n",
        "        mean=mu\n",
        "        variance=var\n",
        "        return out, cache , mean , variance\n",
        "\n",
        "    def batchnorm_backward(self,dout, cache):\n",
        "        #unfold the variables stored in cache\n",
        "        xhat,gamma,xmu,ivar,sqrtvar,var,eps = cache\n",
        "        #get the dimensions of the input/output\n",
        "        N,D = dout.shape\n",
        "        #step9\n",
        "        dbeta = np.sum(dout, axis=0)\n",
        "        dgammax = dout #not necessary, but more understandable\n",
        "        #step8\n",
        "        dgamma = np.sum(dgammax*xhat, axis=0)\n",
        "        dxhat = dgammax * gamma\n",
        "        #step7\n",
        "        divar = np.sum(dxhat*xmu, axis=0)\n",
        "        dxmu1 = dxhat * ivar\n",
        "        #step6\n",
        "        dsqrtvar = -1. /(sqrtvar**2) * divar\n",
        "        #step5\n",
        "        dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar\n",
        "        #step4\n",
        "        dsq = 1. /N * np.ones((N,D)) * dvar\n",
        "        #step3\n",
        "        dxmu2 = 2 * xmu * dsq\n",
        "        #step2\n",
        "        dx1 = (dxmu1 + dxmu2)\n",
        "        dmu = -1 * np.sum(dxmu1+dxmu2, axis=0)\n",
        "        #step1\n",
        "        dx2 = 1. /N * np.ones((N,D)) * dmu\n",
        "        #step0\n",
        "        dx = dx1 + dx2\n",
        "        return dx, dgamma, dbeta\n",
        "\n",
        "    # dropout method\n",
        "    def Dropout(self,W,B,p):\n",
        "        self.M=np.random.binomial(1,p,size=W.shape[1])\n",
        "        # scaled-up by 1/p\n",
        "        W_new=(self.M*(W/p))\n",
        "        B_new=(self.M*(B/p))\n",
        "        return W_new,B_new\n",
        "    #set output layer flag\n",
        "    def is_output_layer(self,boolean):\n",
        "        self.output_layer=boolean\n",
        "    #set open dropout flag\n",
        "    def set_Dropout(self,boolean):\n",
        "        self.dropOut=boolean\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHUwh_jda7w7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP:\n",
        "    \"\"\"\n",
        "    \"\"\"      \n",
        "    def __init__(self, layers, activation,dropout=True):\n",
        "        \"\"\"\n",
        "        :param layers: A list containing the number of units in each layer.\n",
        "        Should be at least two values\n",
        "        :param activation: The activation function to be used. Can be\n",
        "        \"logistic\" or \"tanh\"\n",
        "        \"\"\"        \n",
        "        ### initialize layers\n",
        "        self.layers=[]\n",
        "        self.params=[]\n",
        "        \n",
        "        self.activation=activation\n",
        "        print(len(layers)-1)\n",
        "        for i in range(len(layers)-1):\n",
        "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1]))\n",
        "\n",
        "        self.layers[-1].is_output_layer(True)    \n",
        "\n",
        "        for layer in self.layers:\n",
        "            layer.set_Dropout(dropout) \n",
        "\n",
        "    def forward(self,input,isTest=False,p=0.5):\n",
        "        for layer in self.layers:\n",
        "            output=layer.forward(input,isTest,p=p)\n",
        "            input=output    \n",
        "\n",
        "        return output\n",
        "\n",
        "    def criterion_MSE(self,y,y_hat):\n",
        "\n",
        "        activation_deriv=None\n",
        "        if self.activation[-1]:\n",
        "            activation_deriv=Activation(self.activation[-1]).f_deriv\n",
        "            \n",
        "        error = self.delta_cross_entropy(y_hat,y)\n",
        "        loss= self.cross_entropy( y_hat,y )\n",
        "\n",
        "        # calculate the delta of the output layer   \n",
        "        if activation_deriv:\n",
        "            delta= error * activation_deriv(y_hat)\n",
        "        else:\n",
        "            delta=error\n",
        "        # return loss and delta\n",
        "        return loss,delta\n",
        "        \n",
        "    def backward(self,delta,p=0.5):\n",
        "        # BP progress\n",
        "        delta=self.layers[-1].backward(delta,output_layer=True,p=p)\n",
        "        for layer in reversed(self.layers[:-1]):\n",
        "            delta=layer.backward(delta,p=p)\n",
        "            \n",
        "    def update(self,lr,epoch,opt=None,epsilon=1e-8,weight_decay=False,lamda=1):\n",
        "        for layer in self.layers:\n",
        "            #normal mode\n",
        "            if opt is None:\n",
        "                #using weight decay\n",
        "                if weight_decay==True:\n",
        "                    layer.W = (1-lr*lamda) * layer.W - lr * layer.grad_W\n",
        "                else:\n",
        "                    layer.W -= lr * layer.grad_W\n",
        "                layer.b -= lr * layer.grad_b # why [0] ???\n",
        "\n",
        "                #using weight decay\n",
        "                if weight_decay==True:\n",
        "                    layer.gamma = (1-lr*lamda) * layer.gamma - lr * layer.grad_gamma\n",
        "                else:\n",
        "                    layer.gamma -= lr * layer.grad_gamma\n",
        "                layer.beta -= lr * layer.grad_beta\n",
        "            #momentum mode\n",
        "            elif opt.name=='momentum':\n",
        "                layer.grad_W_past = layer.grad_W_past * opt.momentum + lr * layer.grad_W\n",
        "                #using weight decay\n",
        "                if weight_decay==True:\n",
        "                    layer.W = (1-lr*lamda)*layer.W - layer.grad_W_past \n",
        "                else:\n",
        "                    layer.W -= layer.grad_W_past \n",
        "                layer.grad_b_past=layer.grad_b_past* opt.momentum + lr * layer.grad_b\n",
        "                layer.b -= layer.grad_b_past \n",
        "                \n",
        "\n",
        "                layer.grad_gamma_past = layer.grad_gamma_past * opt.momentum + lr * layer.grad_gamma\n",
        "                #using weight decay\n",
        "                if weight_decay==True:\n",
        "                    layer.gamma = (1-lr*lamda)*layer.gamma - layer.grad_gamma_past \n",
        "                else:\n",
        "                    layer.gamma -= layer.grad_gamma_past \n",
        "                layer.grad_beta_past=layer.grad_beta_past* opt.momentum + lr * layer.grad_beta\n",
        "                layer.beta -= layer.grad_beta_past \n",
        "            #adam mode\n",
        "            elif opt.name=='adam':\n",
        "                layer.W_v = layer.W_v * opt.rho2 + (1-opt.rho2) * (layer.grad_W**2)\n",
        "                W_vt= layer.W_v / (1-opt.rho2**epoch)\n",
        "                layer.W_m = layer.W_m * opt.rho1 + (1-opt.rho1) * layer.grad_W\n",
        "                W_mt= layer.W_m / (1-opt.rho1**epoch)\n",
        "                #using weight decay\n",
        "                if weight_decay==True:\n",
        "                    layer.W = (1-lr*lamda)*layer.W - lr * W_mt / (np.sqrt( W_vt + epsilon ))\n",
        "                else:\n",
        "                    layer.W -= lr * W_mt / (np.sqrt( W_vt + epsilon ))\n",
        "                layer.b_v = layer.b_v * opt.rho2 + (1-opt.rho2) * (layer.grad_b**2)\n",
        "                b_vt= layer.b_v / (1-opt.rho2**epoch)\n",
        "                layer.b_m = layer.b_m * opt.rho1 + (1-opt.rho1) * layer.grad_b\n",
        "                b_mt= layer.b_m / (1-opt.rho1**epoch)\n",
        "                layer.b -= lr * b_mt / (np.sqrt( b_vt + epsilon))\n",
        "\n",
        "                layer.gamma_v = layer.gamma_v * opt.rho2 + (1-opt.rho2) * (layer.grad_gamma**2)\n",
        "                gamma_vt= layer.gamma_v / (1-opt.rho2**epoch)\n",
        "                layer.gamma_m = layer.gamma_m * opt.rho1 + (1-opt.rho1) * layer.grad_gamma\n",
        "                gamma_mt= layer.gamma_m / (1-opt.rho1**epoch)\n",
        "                #using weight decay\n",
        "                if weight_decay==True:\n",
        "                    layer.gamma = (1-lr*lamda)*layer.gamma - lr * gamma_mt / (np.sqrt( gamma_vt + epsilon ))\n",
        "                else:\n",
        "                    layer.gamma -= lr * gamma_mt / (np.sqrt( gamma_vt + epsilon ))\n",
        "                layer.beta_v = layer.beta_v * opt.rho2 + (1-opt.rho2) * (layer.grad_beta**2)\n",
        "                beta_vt= layer.beta_v / (1-opt.rho2**epoch)\n",
        "                layer.beta_m = layer.beta_m * opt.rho1 + (1-opt.rho1) * layer.grad_beta\n",
        "                beta_mt= layer.beta_m / (1-opt.rho1**epoch)\n",
        "                layer.beta -= lr * beta_mt / (np.sqrt( beta_vt + epsilon))\n",
        "\n",
        "         \n",
        "    def fit(self,X,y,learning_rate=0.1, epochs=100,mini_batch_size=100,opt=None,weight_decay=False,wd_rate=1,dropout_rate=0.5):\n",
        "        \"\"\"\n",
        "        Online learning.\n",
        "        :param X: Input data or features\n",
        "        :param y: Input targets\n",
        "        :param learning_rate: parameters defining the speed of learning\n",
        "        :param epochs: number of times the dataset is presented to the network for learning\n",
        "        \"\"\" \n",
        "        X=np.array(X)\n",
        "        y=np.array(y)\n",
        "        to_return = np.zeros(epochs)\n",
        "        train_acc= np.zeros(epochs)\n",
        "        validate_acc= np.zeros(epochs)\n",
        "        \n",
        "        for k in range(epochs):\n",
        "           \n",
        "            ## here is MSGD\n",
        "            mini_batches_X,mini_batches_Y=self.SGD(X,y,mini_batch_size,0)\n",
        "            loss=np.zeros(mini_batches_X.shape[0])\n",
        "\n",
        "            for it in range(mini_batches_X.shape[0]):\n",
        "                # forward pass\n",
        "                y_hat = self.forward(mini_batches_X[it])\n",
        "                # backward pass\n",
        "                loss[it],delta=self.criterion_MSE(mini_batches_Y[it],y_hat)\n",
        "                self.backward(delta)\n",
        "                # update\n",
        "                self.update(lr=learning_rate,opt=opt,epoch=(k*mini_batches_X.shape[0]+it+1),weight_decay=weight_decay,lamda=wd_rate)\n",
        "\n",
        "            to_return[k] = np.mean(loss)\n",
        "            vacc,tacc=self.accuracy_epoch()\n",
        "            train_acc[k]=tacc\n",
        "            validate_acc[k]=vacc\n",
        "            print('ecopch {0} loss: {1:.4f} train_acc: {2:.4f} validate_acc: {3:.4f}'.format(k,float(to_return[k]),tacc,vacc))\n",
        "        return to_return,train_acc,validate_acc\n",
        "    #show each epoch's accuracy\n",
        "    def accuracy_epoch(self):\n",
        "        pre=nn.predict(validate_data)\n",
        "        validate_acc=accuracy_score(validate_label,pre)\n",
        "        pre=nn.predict(train_data)\n",
        "        train_acc=accuracy_score(train_label,pre)\n",
        "        return (validate_acc,train_acc)\n",
        "    #mini-batch SGD method to shuffle and divide dataset\n",
        "    def SGD(sefl,X,Y,mini_batch_size=64,seed=0):\n",
        "       \n",
        "        np.random.seed(seed)\n",
        "        m=X.shape[0]\n",
        "        mini_batches_X=list([])\n",
        "        mini_batches_Y=list([])\n",
        "        #shuffle(XY)\n",
        "        permutation=list(np.random.permutation(m))\n",
        "        shuffl_X=X[permutation,:]\n",
        "        shuffl_Y=Y[permutation]\n",
        "\n",
        "        #Patition and Minus the end case\n",
        "        num_complete_miniBatch=math.floor(m/mini_batch_size)\n",
        "        for k in range(0,num_complete_miniBatch):\n",
        "            mini_batch_X=shuffl_X[k*mini_batch_size:(k+1)*mini_batch_size,:]\n",
        "            mini_batch_Y=shuffl_Y[k*mini_batch_size:(k+1)*mini_batch_size]\n",
        "\n",
        "            mini_batches_X.append(mini_batch_X)\n",
        "            mini_batches_Y.append(mini_batch_Y)\n",
        "        \n",
        "        if not m % mini_batch_size == 0 :\n",
        "            mini_batch_X=shuffl_X[num_complete_miniBatch*mini_batch_size : , : ]\n",
        "            mini_batch_Y=shuffl_Y[num_complete_miniBatch*mini_batch_size : ]\n",
        "\n",
        "            mini_batches_X.append(mini_batch_X)\n",
        "            mini_batches_Y.append(mini_batch_Y)\n",
        "\n",
        "        return np.array(mini_batches_X),np.array(mini_batches_Y)\n",
        "    #prediction stage\n",
        "    def predict(self, x):\n",
        "        x = np.array(x)\n",
        "        output = np.zeros(x.shape[0],dtype=int)\n",
        "        temp=nn.forward(x[:,:],isTest=True)\n",
        "        temp=self.stable_softmax(temp)\n",
        "        output = np.argmax(temp,axis=1)\n",
        "        return output\n",
        "    #softmax in stable mode\n",
        "    def stable_softmax(self,X):\n",
        "        exps=np.exp(X-np.max(X,axis=1,keepdims=True))\n",
        "        return exps/np.sum(exps,axis=1,keepdims=True)\n",
        "    # cross_entropy contains the softMax func\n",
        "    def cross_entropy(self,Y_hat,y):\n",
        "        \"\"\"\n",
        "        X is output of layer\n",
        "        Y is labels \n",
        "        \"\"\"\n",
        "        m = y.shape[0]\n",
        "        p = self.stable_softmax(Y_hat)\n",
        "        log_likelihood = -np.log(p[range(m),y])\n",
        "        # print('log.shape',log_likelihood)\n",
        "        loss = np.sum(log_likelihood)/m   #minibatch=64  caculate the mean loss\n",
        "        return loss\n",
        "\n",
        "    # it contains the softmax deriv func\n",
        "    def delta_cross_entropy(self,Y_hat,y):\n",
        "        m=y.shape[0]\n",
        "        grad=self.stable_softmax(Y_hat)\n",
        "        grad[range(m),y] -= 1\n",
        "        grad = grad/m\n",
        "        return grad\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xv5TRP4vsb-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcdbc794-cc79-4aa7-d0a5-60fb1e52f676"
      },
      "source": [
        "128*1280+1280*640+640*320+320*120+120*32+32*10\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1230400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcUZK1lnblDn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a93d4b3d-f89e-45d1-da6e-8650b679f617"
      },
      "source": [
        "nn = MLP([128,1280,640,320,120,32,10], [None,'leakyrelu','leakyrelu','leakyrelu','leakyrelu','leakyrelu','leakyrelu'],dropout=False)\n",
        "train_data = data[:59000]\n",
        "train_label = label[:59000]\n",
        "validate_data=data[59000:]\n",
        "validate_label=label[59000:]\n",
        "# opt=OPT('momentum',momentum=0.9)\n",
        "opt=OPT('adam',rho2=0.999,rho1=0.9)\n",
        "Loss,train_acc,validate_acc = nn.fit(train_data, train_label, learning_rate=0.0001, epochs=200,mini_batch_size=1000,opt=opt,weight_decay=True,wd_rate=5,dropout_rate=0.5)\n",
        "print('loss:%f'%Loss[-1])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "ecopch 0 loss: 2.0359 train_acc: 0.6766 validate_acc: 0.6610\n",
            "ecopch 1 loss: 1.9358 train_acc: 0.7497 validate_acc: 0.7350\n",
            "ecopch 2 loss: 1.9059 train_acc: 0.7823 validate_acc: 0.7560\n",
            "ecopch 3 loss: 1.8870 train_acc: 0.7988 validate_acc: 0.7680\n",
            "ecopch 4 loss: 1.8712 train_acc: 0.8117 validate_acc: 0.7780\n",
            "ecopch 5 loss: 1.8582 train_acc: 0.8214 validate_acc: 0.7820\n",
            "ecopch 6 loss: 1.8476 train_acc: 0.8278 validate_acc: 0.7870\n",
            "ecopch 7 loss: 1.8384 train_acc: 0.8348 validate_acc: 0.7930\n",
            "ecopch 8 loss: 1.8302 train_acc: 0.8411 validate_acc: 0.7910\n",
            "ecopch 9 loss: 1.8226 train_acc: 0.8465 validate_acc: 0.7970\n",
            "ecopch 10 loss: 1.8157 train_acc: 0.8513 validate_acc: 0.7980\n",
            "ecopch 11 loss: 1.8094 train_acc: 0.8552 validate_acc: 0.8020\n",
            "ecopch 12 loss: 1.8037 train_acc: 0.8589 validate_acc: 0.8060\n",
            "ecopch 13 loss: 1.7985 train_acc: 0.8614 validate_acc: 0.8050\n",
            "ecopch 14 loss: 1.7939 train_acc: 0.8631 validate_acc: 0.8040\n",
            "ecopch 15 loss: 1.7899 train_acc: 0.8648 validate_acc: 0.8050\n",
            "ecopch 16 loss: 1.7867 train_acc: 0.8650 validate_acc: 0.8010\n",
            "ecopch 17 loss: 1.7852 train_acc: 0.8730 validate_acc: 0.8150\n",
            "ecopch 18 loss: 1.7853 train_acc: 0.8707 validate_acc: 0.8150\n",
            "ecopch 19 loss: 1.7829 train_acc: 0.8605 validate_acc: 0.7990\n",
            "ecopch 20 loss: 1.7803 train_acc: 0.8669 validate_acc: 0.8020\n",
            "ecopch 21 loss: 1.7770 train_acc: 0.8735 validate_acc: 0.8120\n",
            "ecopch 22 loss: 1.7747 train_acc: 0.8801 validate_acc: 0.8150\n",
            "ecopch 23 loss: 1.7735 train_acc: 0.8777 validate_acc: 0.8110\n",
            "ecopch 24 loss: 1.7733 train_acc: 0.8779 validate_acc: 0.8060\n",
            "ecopch 25 loss: 1.7740 train_acc: 0.8824 validate_acc: 0.8140\n",
            "ecopch 26 loss: 1.7733 train_acc: 0.8821 validate_acc: 0.8200\n",
            "ecopch 27 loss: 1.7723 train_acc: 0.8846 validate_acc: 0.8170\n",
            "ecopch 28 loss: 1.7711 train_acc: 0.8874 validate_acc: 0.8170\n",
            "ecopch 29 loss: 1.7632 train_acc: 0.9544 validate_acc: 0.8670\n",
            "ecopch 30 loss: 1.7539 train_acc: 0.9604 validate_acc: 0.8680\n",
            "ecopch 31 loss: 1.7485 train_acc: 0.9588 validate_acc: 0.8730\n",
            "ecopch 32 loss: 1.7455 train_acc: 0.9566 validate_acc: 0.8710\n",
            "ecopch 33 loss: 1.7431 train_acc: 0.9681 validate_acc: 0.8800\n",
            "ecopch 34 loss: 1.7400 train_acc: 0.9734 validate_acc: 0.8790\n",
            "ecopch 35 loss: 1.7359 train_acc: 0.9790 validate_acc: 0.8850\n",
            "ecopch 36 loss: 1.7326 train_acc: 0.9808 validate_acc: 0.8830\n",
            "ecopch 37 loss: 1.7308 train_acc: 0.9845 validate_acc: 0.8990\n",
            "ecopch 38 loss: 1.7300 train_acc: 0.9821 validate_acc: 0.8960\n",
            "ecopch 39 loss: 1.7293 train_acc: 0.9859 validate_acc: 0.9040\n",
            "ecopch 40 loss: 1.7279 train_acc: 0.9865 validate_acc: 0.9010\n",
            "ecopch 41 loss: 1.7261 train_acc: 0.9847 validate_acc: 0.8910\n",
            "ecopch 42 loss: 1.7243 train_acc: 0.9908 validate_acc: 0.9010\n",
            "ecopch 43 loss: 1.7227 train_acc: 0.9911 validate_acc: 0.9000\n",
            "ecopch 44 loss: 1.7213 train_acc: 0.9925 validate_acc: 0.9040\n",
            "ecopch 45 loss: 1.7204 train_acc: 0.9931 validate_acc: 0.9020\n",
            "ecopch 46 loss: 1.7195 train_acc: 0.9911 validate_acc: 0.8880\n",
            "ecopch 47 loss: 1.7193 train_acc: 0.9915 validate_acc: 0.8910\n",
            "ecopch 48 loss: 1.7198 train_acc: 0.9913 validate_acc: 0.8870\n",
            "ecopch 49 loss: 1.7203 train_acc: 0.9877 validate_acc: 0.8850\n",
            "ecopch 50 loss: 1.7202 train_acc: 0.9829 validate_acc: 0.8790\n",
            "ecopch 51 loss: 1.7203 train_acc: 0.9831 validate_acc: 0.8790\n",
            "ecopch 52 loss: 1.7196 train_acc: 0.9905 validate_acc: 0.9030\n",
            "ecopch 53 loss: 1.7182 train_acc: 0.9940 validate_acc: 0.9020\n",
            "ecopch 54 loss: 1.7176 train_acc: 0.9934 validate_acc: 0.8960\n",
            "ecopch 55 loss: 1.7164 train_acc: 0.9934 validate_acc: 0.8880\n",
            "ecopch 56 loss: 1.7156 train_acc: 0.9948 validate_acc: 0.8910\n",
            "ecopch 57 loss: 1.7151 train_acc: 0.9943 validate_acc: 0.8970\n",
            "ecopch 58 loss: 1.7145 train_acc: 0.9938 validate_acc: 0.8840\n",
            "ecopch 59 loss: 1.7143 train_acc: 0.9946 validate_acc: 0.8890\n",
            "ecopch 60 loss: 1.7143 train_acc: 0.9960 validate_acc: 0.8990\n",
            "ecopch 61 loss: 1.7144 train_acc: 0.9965 validate_acc: 0.8970\n",
            "ecopch 62 loss: 1.7147 train_acc: 0.9974 validate_acc: 0.9060\n",
            "ecopch 63 loss: 1.7153 train_acc: 0.9957 validate_acc: 0.8900\n",
            "ecopch 64 loss: 1.7164 train_acc: 0.9952 validate_acc: 0.9030\n",
            "ecopch 65 loss: 1.7179 train_acc: 0.9929 validate_acc: 0.9020\n",
            "ecopch 66 loss: 1.7188 train_acc: 0.9882 validate_acc: 0.8850\n",
            "ecopch 67 loss: 1.7200 train_acc: 0.9918 validate_acc: 0.8970\n",
            "ecopch 68 loss: 1.7199 train_acc: 0.9933 validate_acc: 0.8920\n",
            "ecopch 69 loss: 1.7195 train_acc: 0.9924 validate_acc: 0.8900\n",
            "ecopch 70 loss: 1.7187 train_acc: 0.9945 validate_acc: 0.8920\n",
            "ecopch 71 loss: 1.7181 train_acc: 0.9960 validate_acc: 0.8920\n",
            "ecopch 72 loss: 1.7176 train_acc: 0.9973 validate_acc: 0.9050\n",
            "ecopch 73 loss: 1.7172 train_acc: 0.9973 validate_acc: 0.9020\n",
            "ecopch 74 loss: 1.7172 train_acc: 0.9975 validate_acc: 0.8970\n",
            "ecopch 75 loss: 1.7170 train_acc: 0.9977 validate_acc: 0.8930\n",
            "ecopch 76 loss: 1.7174 train_acc: 0.9974 validate_acc: 0.8900\n",
            "ecopch 77 loss: 1.7176 train_acc: 0.9960 validate_acc: 0.8850\n",
            "ecopch 78 loss: 1.7182 train_acc: 0.9958 validate_acc: 0.8960\n",
            "ecopch 79 loss: 1.7183 train_acc: 0.9960 validate_acc: 0.9040\n",
            "ecopch 80 loss: 1.7190 train_acc: 0.9956 validate_acc: 0.8940\n",
            "ecopch 81 loss: 1.7192 train_acc: 0.9945 validate_acc: 0.8860\n",
            "ecopch 82 loss: 1.7199 train_acc: 0.9967 validate_acc: 0.8900\n",
            "ecopch 83 loss: 1.7202 train_acc: 0.9956 validate_acc: 0.8830\n",
            "ecopch 84 loss: 1.7203 train_acc: 0.9935 validate_acc: 0.8740\n",
            "ecopch 85 loss: 1.7209 train_acc: 0.9927 validate_acc: 0.8840\n",
            "ecopch 86 loss: 1.7213 train_acc: 0.9944 validate_acc: 0.8910\n",
            "ecopch 87 loss: 1.7216 train_acc: 0.9958 validate_acc: 0.8930\n",
            "ecopch 88 loss: 1.7219 train_acc: 0.9925 validate_acc: 0.8850\n",
            "ecopch 89 loss: 1.7223 train_acc: 0.9950 validate_acc: 0.8980\n",
            "ecopch 90 loss: 1.7228 train_acc: 0.9948 validate_acc: 0.8880\n",
            "ecopch 91 loss: 1.7230 train_acc: 0.9925 validate_acc: 0.8870\n",
            "ecopch 92 loss: 1.7228 train_acc: 0.9941 validate_acc: 0.8810\n",
            "ecopch 93 loss: 1.7218 train_acc: 0.9963 validate_acc: 0.8830\n",
            "ecopch 94 loss: 1.7214 train_acc: 0.9969 validate_acc: 0.8900\n",
            "ecopch 95 loss: 1.7212 train_acc: 0.9962 validate_acc: 0.8850\n",
            "ecopch 96 loss: 1.7213 train_acc: 0.9959 validate_acc: 0.8870\n",
            "ecopch 97 loss: 1.7212 train_acc: 0.9963 validate_acc: 0.8910\n",
            "ecopch 98 loss: 1.7211 train_acc: 0.9989 validate_acc: 0.8990\n",
            "ecopch 99 loss: 1.7212 train_acc: 0.9986 validate_acc: 0.9020\n",
            "ecopch 100 loss: 1.7215 train_acc: 0.9984 validate_acc: 0.9050\n",
            "ecopch 101 loss: 1.7219 train_acc: 0.9979 validate_acc: 0.8980\n",
            "ecopch 102 loss: 1.7221 train_acc: 0.9989 validate_acc: 0.8990\n",
            "ecopch 103 loss: 1.7224 train_acc: 0.9968 validate_acc: 0.8940\n",
            "ecopch 104 loss: 1.7233 train_acc: 0.9976 validate_acc: 0.8960\n",
            "ecopch 105 loss: 1.7245 train_acc: 0.9935 validate_acc: 0.8820\n",
            "ecopch 106 loss: 1.7275 train_acc: 0.9914 validate_acc: 0.8860\n",
            "ecopch 107 loss: 1.7294 train_acc: 0.9903 validate_acc: 0.8880\n",
            "ecopch 108 loss: 1.7289 train_acc: 0.9900 validate_acc: 0.8750\n",
            "ecopch 109 loss: 1.7285 train_acc: 0.9922 validate_acc: 0.8920\n",
            "ecopch 110 loss: 1.7268 train_acc: 0.9957 validate_acc: 0.8880\n",
            "ecopch 111 loss: 1.7255 train_acc: 0.9946 validate_acc: 0.8830\n",
            "ecopch 112 loss: 1.7252 train_acc: 0.9950 validate_acc: 0.8880\n",
            "ecopch 113 loss: 1.7246 train_acc: 0.9960 validate_acc: 0.8920\n",
            "ecopch 114 loss: 1.7243 train_acc: 0.9990 validate_acc: 0.8880\n",
            "ecopch 115 loss: 1.7240 train_acc: 0.9989 validate_acc: 0.8980\n",
            "ecopch 116 loss: 1.7238 train_acc: 0.9994 validate_acc: 0.8950\n",
            "ecopch 117 loss: 1.7236 train_acc: 0.9993 validate_acc: 0.9030\n",
            "ecopch 118 loss: 1.7237 train_acc: 0.9996 validate_acc: 0.9040\n",
            "ecopch 119 loss: 1.7237 train_acc: 0.9994 validate_acc: 0.9010\n",
            "ecopch 120 loss: 1.7237 train_acc: 0.9992 validate_acc: 0.9000\n",
            "ecopch 121 loss: 1.7237 train_acc: 0.9993 validate_acc: 0.8990\n",
            "ecopch 122 loss: 1.7238 train_acc: 0.9993 validate_acc: 0.9020\n",
            "ecopch 123 loss: 1.7240 train_acc: 0.9982 validate_acc: 0.8890\n",
            "ecopch 124 loss: 1.7244 train_acc: 0.9976 validate_acc: 0.8850\n",
            "ecopch 125 loss: 1.7246 train_acc: 0.9977 validate_acc: 0.8910\n",
            "ecopch 126 loss: 1.7251 train_acc: 0.9977 validate_acc: 0.8920\n",
            "ecopch 127 loss: 1.7258 train_acc: 0.9965 validate_acc: 0.8840\n",
            "ecopch 128 loss: 1.7312 train_acc: 0.9755 validate_acc: 0.8670\n",
            "ecopch 129 loss: 1.7476 train_acc: 0.9758 validate_acc: 0.8770\n",
            "ecopch 130 loss: 1.7396 train_acc: 0.9871 validate_acc: 0.8880\n",
            "ecopch 131 loss: 1.7339 train_acc: 0.9932 validate_acc: 0.8970\n",
            "ecopch 132 loss: 1.7306 train_acc: 0.9940 validate_acc: 0.8870\n",
            "ecopch 133 loss: 1.7294 train_acc: 0.9963 validate_acc: 0.8910\n",
            "ecopch 134 loss: 1.7282 train_acc: 0.9973 validate_acc: 0.8850\n",
            "ecopch 135 loss: 1.7278 train_acc: 0.9976 validate_acc: 0.8880\n",
            "ecopch 136 loss: 1.7273 train_acc: 0.9981 validate_acc: 0.8940\n",
            "ecopch 137 loss: 1.7272 train_acc: 0.9982 validate_acc: 0.8910\n",
            "ecopch 138 loss: 1.7271 train_acc: 0.9974 validate_acc: 0.8850\n",
            "ecopch 139 loss: 1.7269 train_acc: 0.9991 validate_acc: 0.9070\n",
            "ecopch 140 loss: 1.7268 train_acc: 0.9992 validate_acc: 0.9010\n",
            "ecopch 141 loss: 1.7267 train_acc: 0.9989 validate_acc: 0.8980\n",
            "ecopch 142 loss: 1.7266 train_acc: 0.9992 validate_acc: 0.8990\n",
            "ecopch 143 loss: 1.7266 train_acc: 0.9992 validate_acc: 0.9090\n",
            "ecopch 144 loss: 1.7265 train_acc: 0.9991 validate_acc: 0.9070\n",
            "ecopch 145 loss: 1.7265 train_acc: 0.9995 validate_acc: 0.9070\n",
            "ecopch 146 loss: 1.7265 train_acc: 0.9994 validate_acc: 0.9010\n",
            "ecopch 147 loss: 1.7264 train_acc: 0.9996 validate_acc: 0.9020\n",
            "ecopch 148 loss: 1.7266 train_acc: 0.9995 validate_acc: 0.8990\n",
            "ecopch 149 loss: 1.7269 train_acc: 0.9978 validate_acc: 0.8970\n",
            "ecopch 150 loss: 1.7325 train_acc: 0.9823 validate_acc: 0.8810\n",
            "ecopch 151 loss: 1.7438 train_acc: 0.9712 validate_acc: 0.8890\n",
            "ecopch 152 loss: 1.7403 train_acc: 0.9834 validate_acc: 0.8850\n",
            "ecopch 153 loss: 1.7357 train_acc: 0.9906 validate_acc: 0.8910\n",
            "ecopch 154 loss: 1.7330 train_acc: 0.9951 validate_acc: 0.8830\n",
            "ecopch 155 loss: 1.7308 train_acc: 0.9963 validate_acc: 0.8900\n",
            "ecopch 156 loss: 1.7295 train_acc: 0.9969 validate_acc: 0.8880\n",
            "ecopch 157 loss: 1.7292 train_acc: 0.9967 validate_acc: 0.8890\n",
            "ecopch 158 loss: 1.7287 train_acc: 0.9977 validate_acc: 0.8890\n",
            "ecopch 159 loss: 1.7283 train_acc: 0.9995 validate_acc: 0.8990\n",
            "ecopch 160 loss: 1.7281 train_acc: 0.9987 validate_acc: 0.9060\n",
            "ecopch 161 loss: 1.7279 train_acc: 0.9991 validate_acc: 0.8990\n",
            "ecopch 162 loss: 1.7279 train_acc: 0.9995 validate_acc: 0.9000\n",
            "ecopch 163 loss: 1.7277 train_acc: 0.9997 validate_acc: 0.8970\n",
            "ecopch 164 loss: 1.7274 train_acc: 0.9993 validate_acc: 0.9020\n",
            "ecopch 165 loss: 1.7273 train_acc: 0.9999 validate_acc: 0.8990\n",
            "ecopch 166 loss: 1.7272 train_acc: 0.9999 validate_acc: 0.9030\n",
            "ecopch 167 loss: 1.7271 train_acc: 1.0000 validate_acc: 0.9020\n",
            "ecopch 168 loss: 1.7271 train_acc: 0.9999 validate_acc: 0.8980\n",
            "ecopch 169 loss: 1.7270 train_acc: 0.9999 validate_acc: 0.9020\n",
            "ecopch 170 loss: 1.7270 train_acc: 0.9999 validate_acc: 0.9040\n",
            "ecopch 171 loss: 1.7270 train_acc: 0.9999 validate_acc: 0.8980\n",
            "ecopch 172 loss: 1.7270 train_acc: 0.9999 validate_acc: 0.9030\n",
            "ecopch 173 loss: 1.7271 train_acc: 0.9999 validate_acc: 0.8970\n",
            "ecopch 174 loss: 1.7272 train_acc: 0.9999 validate_acc: 0.8980\n",
            "ecopch 175 loss: 1.7278 train_acc: 0.9969 validate_acc: 0.8870\n",
            "ecopch 176 loss: 1.7370 train_acc: 0.9595 validate_acc: 0.8680\n",
            "ecopch 177 loss: 1.7581 train_acc: 0.9648 validate_acc: 0.8760\n",
            "ecopch 178 loss: 1.7459 train_acc: 0.9830 validate_acc: 0.8850\n",
            "ecopch 179 loss: 1.7391 train_acc: 0.9885 validate_acc: 0.8880\n",
            "ecopch 180 loss: 1.7361 train_acc: 0.9904 validate_acc: 0.8950\n",
            "ecopch 181 loss: 1.7345 train_acc: 0.9942 validate_acc: 0.8910\n",
            "ecopch 182 loss: 1.7330 train_acc: 0.9947 validate_acc: 0.8840\n",
            "ecopch 183 loss: 1.7318 train_acc: 0.9961 validate_acc: 0.8940\n",
            "ecopch 184 loss: 1.7312 train_acc: 0.9978 validate_acc: 0.8880\n",
            "ecopch 185 loss: 1.7304 train_acc: 0.9978 validate_acc: 0.8930\n",
            "ecopch 186 loss: 1.7300 train_acc: 0.9972 validate_acc: 0.8820\n",
            "ecopch 187 loss: 1.7297 train_acc: 0.9978 validate_acc: 0.8880\n",
            "ecopch 188 loss: 1.7294 train_acc: 0.9991 validate_acc: 0.8980\n",
            "ecopch 189 loss: 1.7292 train_acc: 0.9994 validate_acc: 0.9060\n",
            "ecopch 190 loss: 1.7289 train_acc: 0.9991 validate_acc: 0.9030\n",
            "ecopch 191 loss: 1.7287 train_acc: 0.9997 validate_acc: 0.8950\n",
            "ecopch 192 loss: 1.7285 train_acc: 0.9997 validate_acc: 0.9080\n",
            "ecopch 193 loss: 1.7284 train_acc: 0.9999 validate_acc: 0.9010\n",
            "ecopch 194 loss: 1.7284 train_acc: 0.9999 validate_acc: 0.9040\n",
            "ecopch 195 loss: 1.7283 train_acc: 0.9998 validate_acc: 0.9030\n",
            "ecopch 196 loss: 1.7283 train_acc: 0.9993 validate_acc: 0.9070\n",
            "ecopch 197 loss: 1.7286 train_acc: 0.9990 validate_acc: 0.8990\n",
            "ecopch 198 loss: 1.7295 train_acc: 0.9973 validate_acc: 0.8970\n",
            "ecopch 199 loss: 1.7301 train_acc: 0.9965 validate_acc: 0.9020\n",
            "loss:1.730101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q-MDEcTeCvu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "c49bd414-479d-4163-83c8-1d7d38436bab"
      },
      "source": [
        "plt.figure(figsize=(15,4))\n",
        "plt.plot(Loss,'r--')\n",
        "plt.plot(train_acc,'b')\n",
        "plt.plot(validate_acc,'g')\n",
        "plt.legend(('Loss','train_accuracy','validate_accuracy'))\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u24ps49rDtK3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e8f89d4-580c-423d-c538-7ff9903d2ea4"
      },
      "source": [
        "pre=nn.predict(train_data)\n",
        "accuracy_score(train_label,pre)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9965254237288136"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NhJ1P9MbHEj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "922e5ef4-7afa-4ca0-c246-8b4a1d596d08"
      },
      "source": [
        "pre=nn.predict(validate_data)\n",
        "accuracy_score(validate_label,pre)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.902"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3YjYkfeXHO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pre_test=nn.predict(test)\n",
        "f = h5py.File('test_label.h5', 'w')\n",
        "f.create_dataset('label', data=pre_test)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwIgXK_1WNN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with h5py.File('/content/test_label.h5','r') as H: \n",
        "    predict = np.copy(H['label']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCbkIzP2L9TA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48ea7751-ebee-45f8-f93b-36a172984b34"
      },
      "source": [
        "predict.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PYjbZXFL-UM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}